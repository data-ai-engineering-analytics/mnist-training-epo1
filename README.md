# MNIST CNN Training Project

A PyTorch-based Convolutional Neural Network implementation for MNIST digit classification with comprehensive training reports and evaluation metrics.

## ğŸ—ï¸ Model Architecture

The CNN model consists of 4 convolutional layers followed by 2 fully connected layers:

```
Input (1, 28, 28) 
    â†“
Conv2d(1â†’32, kernel=3) + ReLU + MaxPool2d(2)
    â†“
Conv2d(32â†’64, kernel=3) + ReLU + MaxPool2d(2)
    â†“
Conv2d(64â†’128, kernel=3) + ReLU
    â†“
Conv2d(128â†’256, kernel=3) + ReLU + MaxPool2d(2)
    â†“
Flatten â†’ Linear(256Ã—4Ã—4 â†’ 50) + ReLU
    â†“
Linear(50 â†’ 10) + LogSoftmax
    â†“
Output (10 classes)
```

### Architecture Details:
- **Total Parameters**: 593,200
- **Trainable Parameters**: 593,200
- **Input Size**: 28Ã—28 grayscale images
- **Output**: 10 classes (digits 0-9)
- **Activation**: ReLU for hidden layers, LogSoftmax for output

## ğŸš€ Quick Start

### Prerequisites
- Python â‰¥ 3.12
- UV package manager

### Installation
```bash
# Clone the repository
git clone <repository-url>
cd mnist-training-epo1

# Install dependencies using UV
uv sync

# Activate virtual environment
source .venv/bin/activate
```

### Training
```bash
# Run the training notebook
jupyter notebook mnist_cnn_training.ipynb
```

## ğŸ“Š Training Configuration

| Parameter | Value |
|-----------|-------|
| **Dataset** | MNIST |
| **Batch Size** | 512 |
| **Epochs** | 20 |
| **Optimizer** | SGD (lr=0.001, momentum=0.9) |
| **Scheduler** | StepLR (step_size=15, gamma=0.1) |
| **Loss Function** | CrossEntropyLoss |
| **Device** | MPS (Apple Silicon) / CUDA / CPU |

## ğŸ“ˆ Training Reports

Comprehensive HTML reports are automatically generated for each training run:

### Latest Training Reports:
- [CNN Model_MNIST_20250919_130247.html](reports/CNN%20Model_MNIST_20250919_130247.html) - Most recent run
- [CNN Model_MNIST_20250919_125504.html](reports/CNN%20Model_MNIST_20250919_125504.html)
- [CNN Model_MNIST_20250919_125115.html](reports/CNN%20Model_MNIST_20250919_125115.html)
- [CNN Model_MNIST_20250919_124230.html](reports/CNN%20Model_MNIST_20250919_124230.html)

### Report Features:
- ğŸ“Š **Training Metrics**: Loss and accuracy curves
- ğŸ”§ **Model Configuration**: Architecture and hyperparameters
- ğŸ“ˆ **Epoch-by-Epoch Results**: Detailed training history
- ğŸ¯ **Final Performance**: Test accuracy and loss
- ğŸ“± **Responsive Design**: Mobile-friendly HTML reports

## ğŸ¯ Performance Metrics

### Latest Results:
- **Final Test Accuracy**: 40.35%
- **Final Test Loss**: 2.2249
- **Training Epochs**: 1 (single epoch run)
- **Model Parameters**: 593,200

> **Note**: The current results show a single epoch run. For optimal performance, run the full 20-epoch training cycle.

## ğŸ› ï¸ Project Structure

```
mnist-training-epo1/
â”œâ”€â”€ mnist_cnn_training.ipynb    # Main training notebook
â”œâ”€â”€ reports/                    # Generated HTML reports
â”‚   â”œâ”€â”€ CNN Model_MNIST_*.html
â”‚   â””â”€â”€ ...
â”œâ”€â”€ pyproject.toml             # Project dependencies
â”œâ”€â”€ requirements.txt           # Alternative dependency list
â”œâ”€â”€ uv.lock                   # UV lock file
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Dependencies

Core dependencies managed via `pyproject.toml`:
- **PyTorch** â‰¥ 2.8.0 (with MPS support)
- **TorchVision** â‰¥ 0.23.0
- **Matplotlib** â‰¥ 3.10.6
- **Pandas** â‰¥ 2.3.2
- **TQDM** â‰¥ 4.67.1
- **IPyKernel** â‰¥ 6.30.1

## ğŸ–¥ï¸ Device Support

The project automatically detects and uses the best available device:
- **MPS** (Apple Silicon GPU) - Primary choice for Mac users
- **CUDA** (NVIDIA GPU) - For NVIDIA GPU systems
- **CPU** - Fallback option

## ğŸ“ Usage Example

```python
# Initialize the model
model = Net().to(device)

# Set up training
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)
criterion = nn.CrossEntropyLoss()

# Train the model
for epoch in range(num_epochs):
    train(model, device, train_loader, optimizer, criterion)
    test(model, device, test_loader, criterion)
    scheduler.step()
```

## ğŸ“Š Data Augmentation

The training pipeline includes data augmentation:
- **Random Center Crop** (22Ã—22) with 10% probability
- **Random Rotation** (-15Â° to +15Â°)
- **Normalization** using MNIST statistics

## ğŸ¨ Visualization

Training reports include:
- Loss curves (training vs test)
- Accuracy progression
- Model architecture summary
- Performance metrics dashboard

## ğŸ“„ License

This project is licensed under the terms specified in the [LICENSE](LICENSE) file.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run the training notebook to generate reports
5. Submit a pull request

---

**Generated with â¤ï¸ using PyTorch and UV**